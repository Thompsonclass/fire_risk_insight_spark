from pyspark.sql import SparkSession

#SparkSession 초기화
spark = SparkSession.builder.appName("FireAnalysis").getOrCreate()

#파일 경로 설정 (2015 ~ 2022년)
base_path = "dbfs:/FileStore/"
file_paths = [base_path + f"{year}_fire_O_utf.csv" for year in range(2015, 2023)]  #2015년부터 2022년까지 파일 리스트

#각 파일의 컬럼 목록을 저장할 리스트
all_columns = []

#각 파일에 대해 컬럼 정보를 추출
for file_path in file_paths:
    df = spark.read.option("header", "true").csv(file_path)
    columns = set(df.columns)  #컬럼 목록을 집합(set)으로 변환하여 중복 제거
    all_columns.append((file_path, columns))

#동일한 컬럼 파악
common_columns = set(all_columns[0][1])  #첫 번째 파일의 컬럼을 기준으로 설정

#모든 파일에 대해 공통 컬럼을 찾음
for _, columns in all_columns[1:]:
    common_columns.intersection_update(columns)  #교집합 업데이트

#각 파일에서 고유한 컬럼 찾기
unique_columns_per_file = {}

for file_path, columns in all_columns:
    unique_columns_per_file[file_path] = columns - common_columns  #공통 컬럼을 제외한 고유 컬럼

print("공통 컬럼들:")
for col in common_columns:
    print(col)

print("\n각 파일의 고유 컬럼들:")
for file_path, unique_columns in unique_columns_per_file.items():
    print(f"\n{file_path}:")
    for col in unique_columns:
        print(f" - {col}")
